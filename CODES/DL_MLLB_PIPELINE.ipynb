{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**COURSE: PRDL/MLLB**\n",
        "\n",
        "**PROJECT: Deep Learning**\n",
        "\n",
        "**TEACHER: Luis Hernández Gómez**\n",
        "\n",
        "**AUTHORS: MARONE Mamadou / RACHIDI Inass**\n",
        "\n",
        "**NOTEBOOK: CUSTOM MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## INSTALLING MODULES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zuPpR2GG9fX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xBvSEAKRwjIj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BertTokenizer, BertModel\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import PorterStemmer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import pipeline\n",
        "from transformers import T5Tokenizer, TFAutoModelForSeq2SeqLM\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYL7jUbxYsX"
      },
      "source": [
        "#  Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(r\"C:\\Users\\maron\\OneDrive\\02-Documents\\00.ETUDES\\00.ECOLE_D_INGE\\00.CYCLE_ING_FORMATION_INIT\\00.3EME_ANNEE_INIT\\00.A_COURS\\00.PRDL\\06.PROJECTS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Nw7PYkAevymd",
        "outputId": "ae6e21cd-784d-4d03-8115-890538f6444f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
              "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
              "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
              "      <td>The New Yorker is taking on President Donald T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         category                                              title  \\\n",
              "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
              "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
              "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
              "\n",
              "                                                body  \n",
              "0  In October 2017, Carolyn Kramer received a dis...  \n",
              "1  This week I talked with actor Jeff Hiller abou...  \n",
              "2  The New Yorker is taking on President Donald T...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned = pd.read_csv(\"DATA\\\\archive\\\\news-article-categories.csv\")\n",
        "df_cleaned.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select just a sample of the data to know have idea about the behaviour of the model during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Unique Categories:\n",
            "ARTS & CULTURE\n",
            "BUSINESS\n",
            "COMEDY\n",
            "CRIME\n",
            "EDUCATION\n",
            "ENTERTAINMENT\n",
            "ENVIRONMENT\n",
            "MEDIA\n",
            "POLITICS\n",
            "RELIGION\n",
            "SCIENCE\n",
            "SPORTS\n",
            "TECH\n",
            "WOMEN\n"
          ]
        }
      ],
      "source": [
        "unique_categories = df_cleaned['category'].unique()\n",
        "\n",
        "# Print the unique categories\n",
        "print(\"All Unique Categories:\")\n",
        "for category in unique_categories:\n",
        "    print(category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_categories = ['ENTERTAINMENT', 'ENVIRONMENT', 'ARTS & CULTURE']\n",
        "\n",
        "# Specify the number of rows you want for each category\n",
        "rows_per_category = 20  # Adjust this number as needed\n",
        "\n",
        "# Filter the DataFrame based on selected categories\n",
        "df_cleaned = df_cleaned[df_cleaned['category'].isin(selected_categories)]\n",
        "\n",
        "# Extract a specified number of rows for each category\n",
        "result_df = pd.DataFrame()\n",
        "for category in selected_categories:\n",
        "    category_subset = df_cleaned[df_cleaned['category'] == category].head(rows_per_category)\n",
        "    result_df = pd.concat([result_df, category_subset], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T1-IrL2g6_uu"
      },
      "outputs": [],
      "source": [
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "df_cleaned['encoded_labels'] = label_encoder.fit_transform(df_cleaned['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into features and labels\n",
        "X = df_cleaned['body']\n",
        "y = df_cleaned['encoded_labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RmWwJssP6fOU"
      },
      "outputs": [],
      "source": [
        "max_chunk_length = 512\n",
        "\n",
        "class CustomClassifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, max_words, max_length,  embedding_dim, hidden_layer_size):\n",
        "        self.max_words = max_words\n",
        "        self.max_length = max_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.tokenizer = Tokenizer(num_words = max_words, oov_token = '<OOV>')\n",
        "        self.model = self._build_model()\n",
        "        self.summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", revision=\"a4f8f3e\")\n",
        "        self.max_chunk_length = 512\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_words, output_dim=self.embedding_dim, input_length=self.max_length))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.hidden_layer_size, activation='relu'))\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    def _count_words(self, text):\n",
        "        words = text.split()\n",
        "        return len(words)\n",
        "\n",
        "    def _summarize(self, text):\n",
        "        # Check if the text is empty or None\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Split the input text into smaller chunks to avoid exceeding the model's maximum sequence length\n",
        "        text_chunks = [text[i:i + max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
        "\n",
        "        # Summarize each chunk separately\n",
        "        summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            # Calculate the maximum length based on 70% of words or a maximum of 100 words\n",
        "            max_length = min(int(self._count_words(chunk) * 0.7), 100)\n",
        "            min_length = min(int(max_length * 0.5), 50)\n",
        "            \n",
        "            try:\n",
        "                summary = self.summarizer(chunk, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "                summaries.append(summary[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarizing chunk: {e}\")\n",
        "                summaries.append(\"\")  # Append an empty string if summarization fails\n",
        "\n",
        "        # Combine the summaries into a single summary\n",
        "        combined_summary = ' '.join(summaries)\n",
        "\n",
        "        return combined_summary\n",
        "\n",
        "\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = text.split()\n",
        "        text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
        "        text = ' '.join(text)\n",
        "        text = text.lstrip(\" \").rstrip(\" \")\n",
        "        return text\n",
        "\n",
        "    def _tokenize_text(self, text):\n",
        "        sequences = self.tokenizer.texts_to_sequences([text])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
        "        return padded_sequences\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_summarized = [self._summarize(article) for article in X]\n",
        "        X_cleaned = [self._clean_text(article) for article in X_summarized]\n",
        "        X_tokenized = [self._tokenize_text(article) for article in X_cleaned]\n",
        "\n",
        "        # Add print statements to check sequence lengths\n",
        "        for article, summarized, cleaned, tokenized in zip(X, X_summarized, X_cleaned, X_tokenized):\n",
        "            print(X_summarized)\n",
        "            print(f\"Original: {len(article)}, Summarized: {len(summarized)}, Cleaned: {len(cleaned)}, Tokenized: {len(tokenized)}\")\n",
        "\n",
        "        self.model.fit(X_tokenized, y, epochs=10, batch_size=32)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_summarized = [self._summarize(article) for article in X]\n",
        "        X_cleaned = [self._clean_text(article) for article in X_summarized]\n",
        "        X_tokenized = [self._tokenize_text(article) for article in X_cleaned]\n",
        "        return X_tokenized\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_tokenized = self.transform(X)\n",
        "        return self.model.predict(X_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U2D2ImBh9Jns"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "max_words = 1000\n",
        "max_length = 200\n",
        "embedding_dim = 50\n",
        "hidden_layer_size = 128\n",
        "simple_classifier = CustomClassifier(max_words, max_length, embedding_dim, hidden_layer_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aKS-eDBB9NTb",
        "outputId": "55560b07-9f35-4143-95dd-3be9cd0f9b38"
      },
      "outputs": [],
      "source": [
        "# Assuming num_classes is defined and X_train, y_train are available\n",
        "simple_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EVALUATION"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
