{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**COURSE: PRDL/MLLB**\n",
        "\n",
        "**PROJECT: Deep Learning**\n",
        "\n",
        "**TEACHER: Luis Hernández Gómez**\n",
        "\n",
        "**AUTHORS: MARONE Mamadou / RACHIDI Inass**\n",
        "\n",
        "**NOTEBOOK: CNN & FFNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## INSTALLING MODULES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1Qi6ID6G2Qv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9ykNmk-MQW2"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRXNhJvpxBzi"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81sgirYcxFOO",
        "outputId": "be07c17a-64c5-41fd-e07a-2a0e69c9ae6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\maron\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYL7jUbxYsX"
      },
      "source": [
        "#  Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.chdir(r\"C:\\Users\\maron\\OneDrive\\02-Documents\\00.ETUDES\\00.ECOLE_D_INGE\\00.CYCLE_ING_FORMATION_INIT\\00.3EME_ANNEE_INIT\\00.A_COURS\\00.PRDL\\06.PROJECTS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Nw7PYkAevymd",
        "outputId": "687aceda-4890-4848-893f-9b7b9598ef03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>model agenc enabl sexual predat year former ag...</td>\n",
              "      <td>octob carolyn kramer receiv disturb phone call...</td>\n",
              "      <td>model agenc enabl sexual predat year former ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>actor jeff hiller talk bright color bold patte...</td>\n",
              "      <td>week talk actor jeff hiller hit broadway play ...</td>\n",
              "      <td>actor jeff hiller talk bright color bold patte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>new yorker cover put trump hole racist comment</td>\n",
              "      <td>new yorker take presid donald trump ask u woul...</td>\n",
              "      <td>new yorker cover put trump hole racist comment...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         category                                              title  \\\n",
              "0  ARTS & CULTURE  model agenc enabl sexual predat year former ag...   \n",
              "1  ARTS & CULTURE  actor jeff hiller talk bright color bold patte...   \n",
              "2  ARTS & CULTURE     new yorker cover put trump hole racist comment   \n",
              "\n",
              "                                                body  \\\n",
              "0  octob carolyn kramer receiv disturb phone call...   \n",
              "1  week talk actor jeff hiller hit broadway play ...   \n",
              "2  new yorker take presid donald trump ask u woul...   \n",
              "\n",
              "                                                text  \n",
              "0  model agenc enabl sexual predat year former ag...  \n",
              "1  actor jeff hiller talk bright color bold patte...  \n",
              "2  new yorker cover put trump hole racist comment...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/PROJET_DL_MLLB/DATA/CLEANED/corpus_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make our model be able to understand the categories we will transform it inot numbers. This action is called label encoding. We will use the LabelEncoder tool provided by Scikit learn to perform it automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t8hPx8Lxc9x"
      },
      "outputs": [],
      "source": [
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "df_cleaned['encoded_labels'] = label_encoder.fit_transform(df_cleaned['category'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can create and separate the features and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into features and labels\n",
        "X = df_cleaned['text']\n",
        "y = df_cleaned['encoded_labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we split the dataset into training and testing sets. The testing set will also be used for validation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEXT PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEWxDkflx02m"
      },
      "source": [
        "## Tokenize and Pad Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uzWN03dx2_h"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "max_words = 5000\n",
        "tokenizer = Tokenizer(num_words = max_words, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert text to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pad sequences for equal length\n",
        "max_length = 200\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen = max_length, padding = 'post', truncating = 'post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K9_x_T8yecV"
      },
      "source": [
        "#  Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0efSbwaLygMF",
        "outputId": "d09614f4-e252-463d-f53b-b30a6ce5d2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\maron\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tApEiK8Oy9fI"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8rocsGoy8RC"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(input_dim = max_words, output_dim = embedding_dim, input_length = max_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ggXLgj0zF2Z"
      },
      "source": [
        "## One dimensionnal Convolutionnal layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCblzefbzMkB"
      },
      "outputs": [],
      "source": [
        "num_filters = 256 #(2**8)\n",
        "filter_size = 3\n",
        "\n",
        "# Convolutional layer\n",
        "model.add(Conv1D(num_filters, filter_size, activation = 'relu'))\n",
        "\n",
        "# Global max pooling layer\n",
        "model.add(GlobalMaxPooling1D())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2UASbhuzRUa"
      },
      "source": [
        "## FFNN Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnVBu3kjzdiU"
      },
      "outputs": [],
      "source": [
        "# Dense layers\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add a dropout to reduce overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model.add(Dense(num_classes, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajuvMDyvznHE"
      },
      "source": [
        "## Additionnal parameters setting & summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLOJyQPyzzsu",
        "outputId": "962df28e-511d-4390-8208-afcbffa8ffa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\maron\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 100)          500000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 198, 256)          77056     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 14)                3598      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 646446 (2.47 MB)\n",
            "Trainable params: 646446 (2.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehtTTZK19Ke"
      },
      "source": [
        "#  Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IU1FbDZ2BFO",
        "outputId": "683c14cb-facc-4b3c-fbc7-62245fab8103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "WARNING:tensorflow:From c:\\Users\\maron\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\maron\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "138/138 [==============================] - 3s 19ms/step - loss: 2.4677 - accuracy: 0.1930 - val_loss: 2.1095 - val_accuracy: 0.3806\n",
            "Epoch 2/7\n",
            "138/138 [==============================] - 2s 18ms/step - loss: 1.6640 - accuracy: 0.4909 - val_loss: 1.2213 - val_accuracy: 0.6322\n",
            "Epoch 3/7\n",
            "138/138 [==============================] - 2s 17ms/step - loss: 1.0057 - accuracy: 0.6827 - val_loss: 1.0039 - val_accuracy: 0.6721\n",
            "Epoch 4/7\n",
            "138/138 [==============================] - 2s 18ms/step - loss: 0.6267 - accuracy: 0.8091 - val_loss: 0.9303 - val_accuracy: 0.6894\n",
            "Epoch 5/7\n",
            "138/138 [==============================] - 2s 17ms/step - loss: 0.3665 - accuracy: 0.8934 - val_loss: 0.9321 - val_accuracy: 0.7175\n",
            "Epoch 6/7\n",
            "138/138 [==============================] - 2s 17ms/step - loss: 0.1834 - accuracy: 0.9591 - val_loss: 0.9555 - val_accuracy: 0.7157\n",
            "Epoch 7/7\n",
            "138/138 [==============================] - 2s 18ms/step - loss: 0.0911 - accuracy: 0.9839 - val_loss: 0.9999 - val_accuracy: 0.7230\n"
          ]
        }
      ],
      "source": [
        "epochs = 7\n",
        "batch_size = 5 #32 #10\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Fit the model with one-hot encoded labels and EarlyStopping callback\n",
        "history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size,validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bHtpX81Q3Fr"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss values\n",
        "plt.plot(history.history['loss'], label = 'Training Loss')\n",
        "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjU07ZM71_PS"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC_kOou02ExC",
        "outputId": "00b2302b-3754-4672-b998-f61fcfad22a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43/43 [==============================] - 0s 5ms/step - loss: 0.9544 - accuracy: 0.7311\n",
            "Test Loss: 0.9543549418449402\n",
            "Test Accuracy: 0.7311046719551086\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict_classes(X_test_padded)\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "# Extract precision, recall, and F1 score\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Weighted Precision:\", precision)\n",
        "print(\"Weighted Recall:\", recall)\n",
        "print(\"Weighted F1 Score:\", f1_score)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INTERPRETATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Text-to-Image Conversion: Convert the input text (news article) into an image-like representation. You can use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings to represent each word or sequence of words as a vector.\n",
        "\n",
        "1D CNN Feature Extraction: Apply a 1D CNN to capture local features in the text representation. This is similar to how a traditional image-based CNN captures features in different regions of an image.\n",
        "\n",
        "Global Average Pooling (GAP): Use Global Average Pooling to condense the extracted features into a single vector. This step is crucial for connecting the CNN features to the subsequent FFNN.\n",
        "\n",
        "Visualization: Visualize the weights of the connections between the last CNN layer and the FFNN layer. Higher weights signify the importance of the corresponding regions in the text. You can overlay these weights onto the original text or create a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace this with your actual input text\n",
        "news_article_text = random.choice(X_train)\n",
        "\n",
        "# Tokenize and pad the input text\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([news_article_text])\n",
        "text_sequence = tokenizer.texts_to_sequences([news_article_text])\n",
        "padded_sequence = pad_sequences(text_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Extract the weights from the convolutional layer\n",
        "conv_layer = model.get_layer('conv1d')  # Replace with the actual name of your Conv1D layer\n",
        "weights = model.get_layer('conv1d').get_weights()[0]\n",
        "\n",
        "# Create a model to get intermediate layer outputs\n",
        "activation_model = Model(inputs=model.input, outputs=model.get_layer('conv1d').output)  # Replace with the actual name of your Conv1D layer\n",
        "\n",
        "# Get the intermediate layer output for the input text\n",
        "activations = activation_model.predict(padded_sequence)\n",
        "\n",
        "# Calculate the importance by multiplying activations with weights\n",
        "cam_output = np.dot(activations, weights)\n",
        "\n",
        "# Visualize the result\n",
        "plt.imshow(cam_output[0].T, cmap='viridis')  # Transpose for better visualization\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
